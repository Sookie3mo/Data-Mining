from pyspark import SparkConf, SparkContext
from collections import defaultdict
from time import time
import sys

def getFreqItems(items, data, support):
	itemSet = set()
	localDict = defaultdict(int)
	for item in items:
		for i in data:
			if item.issubset(i):localDict[item] += 1
	for item, count in localDict.items():
		if count >= support:
			itemSet.add(item)
	return itemSet

def getFreqItemSets(data,support,iters=0):
	ans = dict()
	preCanSet = set()
	dataSets = list()
	# Set a dict to save the counts
	items = defaultdict(lambda: 0)
	# Step1: Count "singletons"
	for line in data:
		lineSet = set(line)
		dataSets.append(lineSet)
		for element in lineSet:
			items[element]+=1

	# fill all frequent items into the answer set
	for item, count in items.iteritems():
		if count >= support:   preCanSet.add(frozenset([item]))
	k = 2
	ans[k-1] = preCanSet
	while preCanSet != set([]) and k != iters+1:
		preCanSet = generateCandidates(preCanSet,k)
		canSet = getFreqItems(preCanSet,dataSets,support)
		if canSet != set([]): ans[k]=canSet
		preCanSet = canSet
		k = k + 1
        return ans

# Generating Candidates
def generateCandidates(itemSet, length):
        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])

# SON map reduce algorithm
def son(input, threshold, sc,caseNum,filename):
    lines = sc.textFile(input)
    # remove header
    header = lines.first()
    rows = lines.filter(lambda x: x != header).map(lambda x: x.split(","))
    #problem 1 works better with 1 partition
    if (input == "Yiming_Liu/input/movies.small2.csv"):
        if caseNum == 1:
            pairs = rows.map(lambda x: (int(x[0]), int(x[1]))).groupByKey(numPartitions = 1).mapValues(list)
        if caseNum == 2:
            pairs = rows.map(lambda x: (int(x[1]), int(x[0]))).groupByKey(numPartitions = 1).mapValues(list)
    else:
        if caseNum == 1:
            pairs = rows.map(lambda x: (int(x[0]), int(x[1]))).groupByKey().mapValues(list)
        if caseNum == 2:
            pairs = rows.map(lambda x: (int(x[1]), int(x[0]))).groupByKey().mapValues(list)

    buckets = pairs.map(lambda (x, y): y).persist()
    #check num of partitions
    numPartitions = buckets.getNumPartitions()
    print numPartitions
    bucketSet = buckets.map(set).persist()

    # Map phase 1
    map1ItemSets = buckets.mapPartitions(lambda data: [x for y in getFreqItemSets(data, threshold / numPartitions).values() for x in y],True)
    allItemSets = map1ItemSets.map(lambda itemsets: (itemsets, 1))

    # Reduce phase1 --- merge the same candidates, but generated by different mappers
    combineCans = allItemSets.reduceByKey(lambda x, y: x).map(lambda (x, y): x)

    # Map phase 2 --- distribute global candidates to all mappers
    combineCans = combineCans.collect()
    candidates = sc.broadcast(combineCans)

    # count candidates in the whole dataset
    counts = bucketSet.flatMap(lambda line: [(candidate, 1) for candidate in candidates.value if line.issuperset(candidate)])

    # reduce phase 2 --- filter finalists
    freqItemSets = counts.reduceByKey(lambda v1, v2: v1 + v2).filter(lambda (i, v): v >= threshold)

    # change output format
    freqItemSets2 = freqItemSets.map(lambda (itemset, count):[x for x in itemset])

    freqItemSets3 = freqItemSets2.map(lambda item: ((len(item),sorted(item)))).sortByKey(lambda x:x).collect()
    writeFile(freqItemSets3, filename)

#write result into txt file with the right format
def writeFile(l, filepath):
    f = open(filepath, 'w')
    a = ''
    b = ''
    alist = []
    blist = []
    for j in range(0, len(l)):
        if l[j][0] == 1:
            alist.append(l[j][1][0])
    alist = sorted(alist)
    for item in alist:
        a += '(' + str(item) + ')' + ','
    a = a[: - 1]
    f.writelines(a)
    f.writelines('\n')
    f.writelines('\n')
    for i in range(2, l[-1][0] + 1):
        for j in range(0, len(l)):
            if l[j][0] == i:
                blist.append(tuple(l[j][1]))
        blist = sorted(blist)
        for item in blist:
            b += str(item) + ','
        b = b[: - 1]
        f.writelines(b)
        b = ''
        blist = []
        f.writelines('\n')
        f.writelines('\n')


if __name__ == "__main__":
    start = time()
    print("Start: " + str(start))
    conf = SparkConf().setMaster("local[*]")
    sc = SparkContext(conf=conf)
    caseNum = int(sys.argv[1])
    input = str(sys.argv[2])
    threshold = int(sys.argv[3])
    filename = str(sys.argv[4])

    son(input, threshold, sc, caseNum,filename)

    sc.stop()
    stop = time()
    print("Stop: " + str(stop))
    print("time: " + str(stop - start) + "s")
